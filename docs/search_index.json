[
["index.html", "The PA Election Relational Database Chapter 1 Predicting the State House, Part 1 1.1 The Relational Database", " The PA Election Relational Database Jonathan Tannen March 20, 2019 Chapter 1 Predicting the State House, Part 1 I’m teaching a workshop at Penn’s Masters of Urban Spatial Analytics program. I’m presenting my work predicting the 2018 State House race. It’s forced me to organize my code, and I think it’s useful enough that I’ll be posting the course resources here as well. This tutorial is organized into four parts: the Relational Database, Processing the GIS Geographies, Prepping the Data for Analysis, and Making the Predictions. In this post, the Relational Database. All of the data is available at https://github.com/jtannen/sth_predictions or on the Penn MUSA GitHub repo 1.1 The Relational Database Before we can talk about modelling, I’ll outline the data as I currently have it organized. I’ve built a “Relational Database” (Chapter 5) out of data from the wonderful Open Elections Project. A relational database consists of separate rectangular datasets. Each dataset has a column or set of columns that provide a unique key. The other data in that row should be only data that applies to that row; you don’t want data in multiple rows that represent the same information. For example, part of my data-base is the data.frame results. The key for this data.frame is four columns: race (e.g. &quot;2016 G STH STH-8&quot;, meaning the competition for the 2016 General, State House district 8) candidate (e.g. &quot;JUDITH D HINES (STH)&quot;) county_code_. and precinct_code, which identify unique precincts. These four values provide unique identifiers for the row. vote_total is the votes received for that row. The dataframe should contain no data that isn’t unique across rows. For example, it doesn’t contain the candidate’s gender (which should be in another data.frame, candidates), or the total turnout for the race (which should be in races). The magic of this setup is that it prevents duplication of data entry or other possible mistakes, while when done well, you can easily join tables to get whatever combination you need. We could join results with candidates to get gender if we need to for a given application. (Note: I’ve already violated the structure above, since results contains party, which is duplicated across precincts and should really belong to the table candidates_to_races, which has unique rows for each candidate and race, ignoring precincts. Rules are made to be broken. _shrugemoji_) library(tidyverse) load(&quot;data/relational_db.rda&quot;) The database has six dataframes. elections has a single row for each election, meaning each year and each primary or general (separating the primaries by party). The column election is the unique identifier: head(elections) ## # A tibble: 6 x 4 ## election_year election_type election cycle ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 2016 P-DEM 2016 P-DEM Presidential ## 2 2016 P-REP 2016 P-REP Presidential ## 3 2016 P- 2016 P- Presidential ## 4 2016 G 2016 G Presidential ## 5 2014 P-REP 2014 P-REP Gubernatorial ## 6 2014 P-DEM 2014 P-DEM Gubernatorial candidates has a single row for each candidate. I’ve done manual cleaning to match up candidates across years who use slightly different names (e.g. different middle initials), so in theory these are unique people. This has one big exception: the same candidate running for different offices will show up as two candidates. This felt right to me, it will later mean that success in a State House race doesn’t inform success is a Gubernatorial race. The table is actually unique candidate + office combinations. In candidates, column names is a list of all of the names that candidate has used on a ballot. Column parties is the list of parties the candidate has ever run as, and party_guess is my guess at whether a candidate is one of (Democrat, Republican), which I will use when they appear on the ballot as a third party. This is usually easy: a candidate who ran as a Republican four years ago might run as D/R if they’re unopposed. If they’ve never run as a Dem or a Rep, I leave them as the third party on the ballot. The column gender is just my guess of the candidate’s gender based on their name/googling. (Let me know if you find any issues!). The column candidate is the unique identifier: head(candidates) ## # A tibble: 6 x 6 ## office candidate names parties party_guess gender ## &lt;chr&gt; &lt;chr&gt; &lt;list&gt; &lt;list&gt; &lt;chr&gt; &lt;fct&gt; ## 1 GOV ALLYSON Y SCHWARTZ (GOV) &lt;chr [3 x ~ &lt;chr [1~ DEM F ## 2 GOV ANTHONY HARDY WILLIAMS (~ &lt;chr [3 x ~ &lt;chr [1~ DEM M ## 3 GOV BOB CASEY (GOV) &lt;chr [3 x ~ &lt;chr [1~ DEM M ## 4 GOV DAN ONORATO (GOV) &lt;chr [3 x ~ &lt;chr [2~ DEM M ## 5 GOV ED RENDELL (GOV) &lt;chr [3 x ~ &lt;chr [4~ DEM M ## 6 GOV JACK WAGNER (GOV) &lt;chr [3 x ~ &lt;chr [1~ DEM M Elections are divided into races. Each race is the competition for a given office in a given election. State-wide races are given the district PA. I populate the column is_contested as whether the second-place candidate won at least 5% of the vote; the goal is to identify whether two candidates’ names were actually on the ballot, and rule out write-ins. All entries in the .*_candidate columns match the unique identifier in candidates. The column race is the unique identifier here: head(races) ## # A tibble: 6 x 15 ## election office district is_contested race_turnout race winner_party ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;lgl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 2002 G GOV GOV-PA TRUE 3580986 2002~ DEM ## 2 2002 G STH STH-1 TRUE 12238 2002~ DEM ## 3 2002 G STH STH-10 FALSE 14724 2002~ REP ## 4 2002 G STH STH-100 TRUE 13605 2002~ REP ## 5 2002 G STH STH-101 TRUE 16806 2002~ REP ## 6 2002 G STH STH-102 TRUE 18222 2002~ REP ## # ... with 8 more variables: winner_candidate &lt;chr&gt;, ## # winner_pct_vote &lt;dbl&gt;, is_demrep &lt;lgl&gt;, dem_candidate &lt;chr&gt;, ## # dem_votes &lt;dbl&gt;, rep_candidate &lt;chr&gt;, rep_votes &lt;dbl&gt;, ## # pct_dem_2party &lt;dbl&gt; The dataframe candidates_to_races maps candidates to races. This includes their total candidate_race_votes, the party that they were listed on the ballot as for this race as well as party_replaced, which imputes Dem/Rep using party_guess if they’re listed as a third party and there are no other Dems or no other Reps. Note: is_incumbent indicates whether the candidate won any seat for this office in the last cycle, regardless of district (which is important since there was redistricting in 2012.) For example, I consider Jason Altmire an incumbent in his 2012 race for USC-12, even though two years earlier he won in USC-4. head(candidates_to_races) ## # A tibble: 6 x 7 ## race candidate party candidate_race_~ is_incumbent party_guess ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;chr&gt; ## 1 2002~ ED RENDE~ DEM 1911587 NA DEM ## 2 2002~ KEN V KR~ LIB 40918 NA LIB ## 3 2002~ MICHAEL ~ GRN 38378 NA GRN ## 4 2002~ MIKE FIS~ REP 1589030 NA REP ## 5 2002~ BILL STE~ REP 3343 NA REP ## 6 2002~ LINDA BE~ DEM 8895 NA DEM ## # ... with 1 more variable: party_replaced &lt;chr&gt; The table precincts_to_districts is a mapping of which precincts voted for which districts in each year. I’ll discuss this (and why we need it) in Processing GIS. Finally, results is the good stuff: the precinct-level results. head(results) ## # A tibble: 6 x 6 ## race candidate county_code_. precinct_code party vote_total ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 2016 P-DEM ~ HILLARY CLINT~ 1 10 DEM 37 ## 2 2016 P-DEM ~ HILLARY CLINT~ 1 20 DEM 42 ## 3 2016 P-DEM ~ HILLARY CLINT~ 1 30 DEM 13 ## 4 2016 P-DEM ~ HILLARY CLINT~ 1 40 DEM 105 ## 5 2016 P-DEM ~ HILLARY CLINT~ 1 50 DEM 35 ## 6 2016 P-DEM ~ HILLARY CLINT~ 1 60 DEM 59 That’s it! Now for the hard part: Processing the GIS geographies. "],
["predicting-the-state-house-part-2.html", "Chapter 2 Predicting the State House, Part 2 2.1 Creating Geographies 2.2 Year to year crosswalks", " Chapter 2 Predicting the State House, Part 2 2.1 Creating Geographies First, let’s load the Relational Database. library(tidyverse) source(&quot;utils/util.R&quot;) load(&quot;data/relational_db.rda&quot;) The results dataframe contains precinct-level results. For our model, we are going to need to compare these results from year to year (for example, to use last election’s results to predict this one). This can be a problem. Districts can, and do, change from year to year. There are two ways this can happen, one measurable and one not. First, the measurable: the state legislature passed new boundaries following the 2010 US Census. Thus, the districts from 2010 and before are completely different from those 2014 and after. The middle year, 2012, was particularly complicated. The Supreme Court had struck down the State House (STH) and State Senate (STS) boundaries, but allowed the US Congress (USC) boundaries; that election used the pre-2012 boundaries for state elections, post-2012 boundaries for USC. And then in 2018, of course, the PA Supreme Court redrew the USC boundaries, but not the state ones. Here’s a table of elections to vintage of districts: Election State Boundaries USC Boundaries 2010 and before pre-2012 pre-2012 2012 pre-2012 post-2012 2014-2016 post-2012 post-2012 2018 post-2012 2018 But that’s all measurable. We can (and will) use GIS to create crosswalks from one to the other. More pernicious is that municipalities seem to arbitrarily change the names and boundaries of their precincts. So precinct 1 in a township in 2004 possibly has zero overlap with that precinct in 2006. Ideally, we would have a shapefile of the precincts and could map them out. But we don’t. As far as I can tell, the only requirement for municipalities is to provide pdfs or even textual descriptions of their new precincts (e.g. &quot;South of Pine Street, down to Maple St.&quot;). After gallantly downloading a few dozen pdfs, I gave up on attempts to map this out. (Aside: There’s a great project to collect and create shapefiles for all precincts used in the 2016 election, but even that would only be for a single year, and not the full set of precincts we need.) The table precincts_to_districts contains a matching of each precinct and what STH, STS, and USC districts they voted for in a given election. This is pulled entirely from the results table; there’s no GIS used. For example, here are precinct ids that changed State House districts between 2014 and 2016. Since the STH districts didn’t change that year, the only explanation is that the precinct boundaries are in totally new places. Four precincts in Delaware County appear to have swapped Congressional and State House districts. This could be an actual change in the map, a data error, who knows. inner_join( precincts_to_districts %&gt;% filter(election == &quot;2014 G&quot;), precincts_to_districts %&gt;% filter(election == &quot;2016 G&quot;), by=c(&#39;county_code_.&#39;, &#39;precinct_code&#39;, &#39;vtd_code&#39;) ) %&gt;% filter(sth.x != sth.y) %&gt;% as.data.frame() ## election.x county_code_. precinct_code vtd_code cofips.x usc.x sts.x ## 1 2014 G 23 3350 3370 045 01 026 ## 2 2014 G 23 3380 3400 045 01 026 ## 3 2014 G 23 3470 3490 045 07 026 ## 4 2014 G 23 3510 3530 045 07 026 ## sth.x election.y cofips.y usc.y sts.y sth.y ## 1 164 2016 G 045 07 026 163 ## 2 164 2016 G 045 07 026 163 ## 3 163 2016 G 045 01 026 164 ## 4 163 2016 G 045 01 026 164 The one thing that I do trust is the county and STH, STS, and USC boundaries. So we’ll use those as our anchor. In other words, if precinct code 3350 voted for Congressional District 1 in 2014, and for CD 7 in 2016, we’ll assume that the boundaries moved, and not that the same people voted for different districts. So let’s create our own geographies. The idea is that we will identify unique combinations of STH, STS, USC, and county boundaries using shapefiles. We can then confidently map the precinct results to them, because we know which STH, STS, and USC each precinct voted for in a given election. These geographies will thus be larger than precincts, but smaller than STH boundaries (potentially equal to, if an STH is entirely contained withing STS, USC, and county). I call these, imaginatively, geographies. We will need four vintages of these: pre-2012, 2012, post-2012, and 2018 (see the table above). Let’s load the shapefiles, and calculate the intersections of the four geometries: ## Warning, I haphazardly move between sf and sp packages, ## based on my preferred function. library(sf) library(rgeos) library(rgdal) library(sp) sth_pre2012 &lt;- st_read(&quot;data/state_house/tigris_lower_house_2011.shp&quot;, quiet=TRUE) %&gt;% mutate(sth = SLDLST) sth_post2012 &lt;- st_read(&quot;data/state_house/tigris_lower_house_2015.shp&quot;, quiet=TRUE) %&gt;% mutate(sth = SLDLST) sts_pre2012 &lt;- st_read(&quot;data/state_senate/tigris_upper_house_2011.shp&quot;, quiet=TRUE) %&gt;% mutate(sts = SLDUST) sts_post2012 &lt;- st_read(&quot;data/state_senate/tigris_upper_house_2015.shp&quot;, quiet=TRUE) %&gt;% mutate(sts = SLDUST) usc_pre2012 &lt;- st_read(&quot;data/congress/tigris_usc_2011.shp&quot;, quiet=TRUE) %&gt;% mutate(usc = CD111FP) usc_post2012 &lt;- st_read(&quot;data/congress/tigris_usc_2015.shp&quot;, quiet=TRUE) %&gt;% mutate(usc = CD114FP) usc_2018 &lt;- st_read(&quot;data/congress/supcourt_usc_2018.shp&quot;, quiet=TRUE) %&gt;% mutate(usc = DISTRICT) counties &lt;- st_read(&quot;data/census/tigris_counties_2015.shp&quot;, quiet=TRUE) %&gt;% mutate(cofips = COUNTYFP) ## some helper functions get_sp_id &lt;- function(s) sapply(s@polygons, slot, &quot;ID&quot;) gInt_byid &lt;- function(x, y, sep=&quot;_&quot;){ int &lt;- gIntersection(x, y, byid=TRUE, drop_lower_td=TRUE) int &lt;- gBuffer(int, byid=TRUE, width=0) int &lt;- spChFIDs(int, gsub(&quot;\\\\s&quot;, sep, get_sp_id(int))) return(int) } sf_to_sp &lt;- function(sf, idcol){ sf &lt;- as(sf, &quot;Spatial&quot;) sf &lt;- spChFIDs(sf, as.character(sf@data[,idcol])) return(sf) } get_intersection &lt;- function(sth, sts, usc, counties, vintage){ ## I find the sf intersection functions hard to work with, so we&#39;ll use sp sth &lt;- sf_to_sp(sth, &#39;sth&#39;) sts &lt;- sf_to_sp(sts, &#39;sts&#39;) usc &lt;- sf_to_sp(usc, &#39;usc&#39;) counties &lt;- sf_to_sp(counties, &#39;cofips&#39;) sth_sts &lt;- gInt_byid(sth, sts) sth_sts_usc &lt;- gInt_byid(sth_sts, usc) sth_sts_usc_co &lt;- gInt_byid(sth_sts_usc, counties) sth_sts_usc_co &lt;- spChFIDs( sth_sts_usc_co, paste0(vintage, &quot;_&quot;, row.names(sth_sts_usc_co)) ) id_df &lt;- data.frame(GEOID = row.names(sth_sts_usc_co)) %&gt;% separate( GEOID, into=c(&#39;vintage&#39;, &#39;sth&#39;, &#39;sts&#39;, &#39;usc&#39;,&#39;cofips&#39;), sep=&quot;_&quot;, remove=FALSE ) sth_sts_usc_co &lt;- SpatialPolygonsDataFrame( sth_sts_usc_co, id_df, match.ID=FALSE ) sth_sts_usc_co &lt;- st_as_sf(sth_sts_usc_co) return(sth_sts_usc_co) } ## get and store the geographies geographies &lt;- list() geographies[[&#39;pre2012&#39;]] &lt;- get_intersection( sth_pre2012, sts_pre2012, usc_pre2012, counties, &quot;pre2012&quot; ) geographies[[&#39;2012&#39;]] &lt;- get_intersection( sth_pre2012, sts_pre2012, usc_post2012, counties, &quot;2012&quot; ) geographies[[&#39;post2012&#39;]] &lt;- get_intersection( sth_post2012, sts_post2012, usc_post2012, counties, &quot;post2012&quot; ) geographies[[&#39;2018&#39;]] &lt;- get_intersection( sth_post2012, sts_post2012, usc_2018, counties, &quot;2018&quot; ) ## create a single table to rule them all geographies &lt;- do.call(rbind, geographies) ## one useful table matching years to their vintage year_to_geo_vintage &lt;- data.frame( year = as.character(seq(2002, 2018, 2)), vintage = c(rep(&quot;pre2012&quot;, 5), &quot;2012&quot;, rep(&quot;post2012&quot;, 2), &quot;2018&quot;), stringsAsFactors = FALSE ) It’s worth noting that geographies are weird. They are wildly different sizes, and in general represent vastly different populations. One geography might be an entire state house district (if it’s nested within a state senate, congressional, and county boundary). Another might be a tiny sliver with no population, where a nub of one district crosses into a different county. We’ll need to keep this in mind when we model them later. For example, here are the 2014-2016 geographies for Philadelphia: ggplot( geographies %&gt;% filter(vintage == &#39;post2012&#39; &amp; cofips == 101) ) + geom_sf() + theme_map_sixtysix() To be able to ignore precincts forever hereafter, let’s aggregate the results from precincts to geographies. tmp_result_list &lt;- list() for(year in seq(2002, 2016, 2)){ vintage &lt;- year_to_geo_vintage$vintage[year_to_geo_vintage$year == year] geo &lt;- geographies[geographies$vintage == vintage,] %&gt;% as.data.frame() %&gt;% select(-geometry) tmp_result &lt;- results %&gt;% filter(substr(race, 1, 4) == year) %&gt;% left_join(races %&gt;% select(race, election)) %&gt;% left_join(precincts_to_districts) %&gt;% left_join(geo) %&gt;% group_by(race, candidate, party, cofips, usc, sts, sth) %&gt;% summarise(vote_total = sum(vote_total)) %&gt;% group_by() %&gt;% mutate( GEOID = paste(vintage, sth, sts, usc, cofips, sep=&quot;_&quot;) ) tmp_result_list[[as.character(year)]] &lt;- tmp_result } geography_results &lt;- bind_rows(tmp_result_list) head(geography_results) ## # A tibble: 6 x 9 ## race candidate party cofips usc sts sth vote_total GEOID ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 2002 G ~ ED RENDELL~ DEM 001 19 033 091 5393 pre2012_~ ## 2 2002 G ~ ED RENDELL~ DEM 001 19 033 193 2339 pre2012_~ ## 3 2002 G ~ ED RENDELL~ DEM 003 04 037 016 698 pre2012_~ ## 4 2002 G ~ ED RENDELL~ DEM 003 04 037 027 876 pre2012_~ ## 5 2002 G ~ ED RENDELL~ DEM 003 04 037 044 2024 pre2012_~ ## 6 2002 G ~ ED RENDELL~ DEM 003 04 038 024 503 pre2012_~ Let’s do a quick check to make sure we didn’t duplicate any votes: usp_2016 &lt;- &quot;2016 G USP USP-PA&quot; new_results &lt;- geography_results %&gt;% filter(race == usp_2016) %&gt;% group_by(candidate) %&gt;% summarise(new_votes = sum(vote_total)) old_results &lt;- results %&gt;% filter(race == usp_2016) %&gt;% group_by(candidate) %&gt;% summarise(old_votes = sum(vote_total)) left_join(new_results, old_results) ## # A tibble: 5 x 3 ## candidate new_votes old_votes ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 DARRELL L CASTLE (USP) 21566 21566 ## 2 DONALD J TRUMP (USP) 2970378 2970378 ## 3 GARY E JOHNSON (USP) 146659 146659 ## 4 HILLARY CLINTON (USP) 2925758 2925758 ## 5 JILL STEIN (USP) 49935 49935 Yep, Donald Trump still won. 2.2 Year to year crosswalks We’re going to want to compare the results from one year to the next, so we need to map vintages of the geographies to each other. In order to do that, let’s build a crosswalk. People often use areal crosswalks, but we’ll do one better with population-weighted crosswalks. The strategy is to create the geographic intersections of geographies x and y, use Census Blocks to calculate the population in each intersection, and then apportion votes in x based on the fraction of its population that overlaps with y. We’ll use blocks from the 2010 Census. Using populations from a single year won’t accurately weight places where population are changing, but will provide a darn good first order approximation. (You could imagine being even better, weighting by voting age population, or even downloading the voter file and weighting by voters). block_pops &lt;- read_csv( &quot;data/census/block10_centroid_pops.csv&quot;, col_types = cols(GEOID10 = col_character()) ) ## get rid of NA pop blocks. (I have no idea what these are, but they have 0 ALAND) block_pops %&gt;% filter(is.na(pop10)) ## # A tibble: 7 x 9 ## STATEFP10 COUNTYFP10 TRACTCE10 BLOCKCE10 GEOID10 pop10 ALAND10 INTPTLAT10 ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 42 049 990000 2 420499~ NA 0 42.2 ## 2 42 049 990000 6 420499~ NA 0 42.2 ## 3 42 049 990000 1 420499~ NA 0 42.3 ## 4 42 049 990000 3 420499~ NA 0 42.2 ## 5 42 049 990000 4 420499~ NA 0 42.1 ## 6 42 049 012400 1 420490~ NA 0 42.1 ## 7 42 049 990000 5 420499~ NA 0 42.2 ## # ... with 1 more variable: INTPTLON10 &lt;dbl&gt; block_pops &lt;- block_pops %&gt;% filter(!is.na(pop10)) block_cents &lt;- SpatialPointsDataFrame( coords=block_pops[,c(&#39;INTPTLON10&#39;,&#39;INTPTLAT10&#39;)], data=block_pops, coords.nrs=match(c(&#39;INTPTLON10&#39;,&#39;INTPTLAT10&#39;), names(block_pops)), proj4string=CRS(&quot;+init=epsg:4326&quot;) ) %&gt;% st_as_sf %&gt;% st_transform(st_crs(geographies)) create_crosswalk &lt;- function( x, y, blocks=block_cents, xid=&#39;GEOID&#39;, yid=&#39;GEOID&#39; ){ x &lt;- sf_to_sp(x, xid) y &lt;- sf_to_sp(y, yid) id_sep &lt;- &quot;:&quot; int &lt;- gInt_byid(x, y, id_sep) blocks_to_int &lt;- blocks %&gt;% as(&quot;Spatial&quot;) %&gt;% over(int) # Provide a sanity check that all blocks got mapped. print(paste(&quot;We lost&quot;, sum(is.na(blocks_to_int)), &quot;blocks.&quot;)) print(paste(&quot;We lost&quot;, sum(blocks$pop10[is.na(blocks_to_int)]), &quot;people.&quot;)) crosswalk &lt;- data.frame( int_num = blocks_to_int, pop = blocks$pop10 ) %&gt;% mutate(int_id = get_sp_id(int)[int_num]) %&gt;% group_by(int_id) %&gt;% summarise(pop = sum(pop)) %&gt;% group_by() %&gt;% separate(int_id, into=c(&quot;xid&quot;, &quot;yid&quot;), sep=id_sep) %&gt;% group_by(xid) %&gt;% mutate(frac_of_x = pop / sum(pop)) %&gt;% group_by(yid) %&gt;% mutate(frac_of_y = pop / sum(pop)) ## check that nothing got duplicated/lost. print(paste(&quot;Pop of blocks:&quot;, sum(blocks$pop10))) print(paste(&quot;Pop of cw: &quot;, with(crosswalk, sum(crosswalk$pop)))) return(crosswalk) } crosswalks &lt;- list() for( vintage_pair in c( &quot;pre2012,2012&quot;, &quot;pre2012,post2012&quot;, &quot;2012,post2012&quot;, &quot;post2012,2018&quot; ) ){ print(vintage_pair) vintages &lt;- strsplit(vintage_pair, &quot;,&quot;)[[1]] crosswalks[[vintage_pair]] &lt;- create_crosswalk( geographies %&gt;% filter(vintage == vintages[1]), geographies %&gt;% filter(vintage == vintages[2]) ) } ## [1] &quot;pre2012,2012&quot; ## [1] &quot;We lost 2 blocks.&quot; ## [1] &quot;We lost 0 people.&quot; ## [1] &quot;Pop of blocks: 12702379&quot; ## [1] &quot;Pop of cw: 12702379&quot; ## [1] &quot;pre2012,post2012&quot; ## [1] &quot;We lost 2 blocks.&quot; ## [1] &quot;We lost 0 people.&quot; ## [1] &quot;Pop of blocks: 12702379&quot; ## [1] &quot;Pop of cw: 12702379&quot; ## [1] &quot;2012,post2012&quot; ## [1] &quot;We lost 0 blocks.&quot; ## [1] &quot;We lost 0 people.&quot; ## [1] &quot;Pop of blocks: 12702379&quot; ## [1] &quot;Pop of cw: 12702379&quot; ## [1] &quot;post2012,2018&quot; ## [1] &quot;We lost 0 blocks.&quot; ## [1] &quot;We lost 0 people.&quot; ## [1] &quot;Pop of blocks: 12702379&quot; ## [1] &quot;Pop of cw: 12702379&quot; Here’s an example of how to use the crosswalk. Suppose we wanted to map the results in 2012 to the geographies in 2014. We want to assign to the 2014 boundaries the weighted average of the results from 2012: geography_results %&gt;% inner_join( races %&gt;% filter(election == &#39;2012 G&#39;) ) %&gt;% left_join( crosswalks[[&quot;2012,post2012&quot;]], by = c(&quot;GEOID&quot; = &quot;xid&quot;) ) %&gt;% group_by(election, yid, office, party) %&gt;% summarise( vote_total = sum(vote_total * frac_of_x) ) ## # A tibble: 7,185 x 5 ## # Groups: election, yid, office [?] ## election yid office party vote_total ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 2012 G post2012_001_049_03_049 STH DEM 15907. ## 2 2012 G post2012_001_049_03_049 STH OTH 0 ## 3 2012 G post2012_001_049_03_049 STS DEM 13708. ## 4 2012 G post2012_001_049_03_049 STS OTH 0 ## 5 2012 G post2012_001_049_03_049 STS REP 4480. ## 6 2012 G post2012_001_049_03_049 USC DEM 12831. ## 7 2012 G post2012_001_049_03_049 USC IND 909. ## 8 2012 G post2012_001_049_03_049 USC OTH 0 ## 9 2012 G post2012_001_049_03_049 USC REP 4371. ## 10 2012 G post2012_001_049_03_049 USP DEM 14046. ## # ... with 7,175 more rows Finally, let’s use the crosswalks to add a quick feature to geographies: the 2010 population. (This doesn’t actually require the crosswalks–we could just map the blocks to a single time period–but since we already have them…) get_pop_from_cw &lt;- function(vintage_pair, vintage){ vintage_split &lt;- str_split(vintage_pair, &quot;,&quot;)[[1]] x_or_y &lt;- c(&quot;xid&quot;, &quot;yid&quot;)[which(vintage_split == vintage)] if(length(x_or_y) != 1) stop(&quot;vintage doesn&#39;t match&quot;) cw &lt;- crosswalks[[vintage_pair]] cw$GEOID &lt;- cw[,x_or_y] %&gt;% unlist return(cw %&gt;% group_by(GEOID) %&gt;% summarise(pop10 = sum(pop, na.rm = TRUE))) } pops_pre2012 &lt;- get_pop_from_cw(&quot;pre2012,2012&quot;, &quot;pre2012&quot;) pops_2012 &lt;- get_pop_from_cw(&quot;pre2012,2012&quot;, &quot;2012&quot;) pops_post2012 &lt;- get_pop_from_cw(&quot;2012,post2012&quot;, &quot;post2012&quot;) pops_2018 &lt;- get_pop_from_cw(&quot;post2012,2018&quot;, &quot;2018&quot;) geography_pops &lt;- bind_rows(pops_pre2012, pops_2012, pops_post2012, pops_2018) geographies &lt;- left_join(geographies, geography_pops) Let’s compare the total votes to the total population to make sure nothing crazy happened: uspgov_votes &lt;- geography_results %&gt;% inner_join( races %&gt;% filter( office %in% c(&quot;GOV&quot;, &quot;USP&quot;) &amp; substr(election, 6, 6) == &quot;G&quot; ) ) %&gt;% mutate(year = substr(election, 1, 4)) %&gt;% group_by(year, GEOID) %&gt;% summarise(vote_total = sum(vote_total)) ggplot( uspgov_votes %&gt;% left_join(geography_pops), aes(x=pop10, y=vote_total) ) + geom_point() + theme_sixtysix() That looks good, but I want to flag something that’s weird. There are some combinations of districts that the shapefiles say shouldn’t exist. Here are some votes that aren’t in our intersections. ## Looks uspgov_votes %&gt;% filter(!GEOID %in% geography_pops$GEOID) ## # A tibble: 40 x 3 ## # Groups: year [8] ## year GEOID vote_total ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 2002 pre2012_046_017_07_091 430 ## 2 2002 pre2012_058_045_12_129 1754 ## 3 2002 pre2012_058_045_18_129 106 ## 4 2002 pre2012_136_016_15_095 1368 ## 5 2002 pre2012_137_016_15_095 1672 ## 6 2002 pre2012_175_001_03_101 0 ## 7 2004 pre2012_058_045_12_129 2770 ## 8 2004 pre2012_058_045_18_129 155 ## 9 2004 pre2012_136_016_15_095 2223 ## 10 2004 pre2012_137_016_15_095 2889 ## # ... with 30 more rows Particularly, let’s consider pre2012_136_016_15_095. There are 1,618 votes in 2006 in precincts voted for State House 137 and State Senate 16. According to the census maps, those districts don’t overlap. I haven’t been able to figure out if this is a problem with the vote data or the shapefiles, and we’ll ignore it for the time being. How many votes do we mess up? uspgov_votes %&gt;% filter(!GEOID %in% geography_pops$GEOID) %&gt;% group_by(year) %&gt;% summarise(sum(vote_total)) ## # A tibble: 8 x 2 ## year `sum(vote_total)` ## &lt;chr&gt; &lt;dbl&gt; ## 1 2002 5330 ## 2 2004 8037 ## 3 2006 6796 ## 4 2008 9332 ## 5 2010 6056 ## 6 2012 13250 ## 7 2014 1 ## 8 2016 1 Our lost votes come entirely in 2012 or before, with a max of 13,000 votes in 2012. Does this matter? I have no idea. We’re done for now. Let’s save it and do some analysis! if(!dir.exists(&quot;outputs&quot;)) dir.create(&quot;outputs&quot;) save_objs &lt;- c(&quot;crosswalks&quot;,&quot;geographies&quot;,&quot;geography_results&quot;, &quot;year_to_geo_vintage&quot;) save( list = save_objs, file=&quot;outputs/geographies_output.rda&quot; ) "],
["predicting-the-state-house-part-3.html", "Chapter 3 Predicting the State House, Part 3 3.1 Prepping the data for analysis 3.2 Thinking ahead 3.3 The model 3.4 Building 2018", " Chapter 3 Predicting the State House, Part 3 3.1 Prepping the data for analysis After having made such a big deal about Relational Databases, we need to create a boring old rectangular data.frame for our regression analysis. Let’s do that here. Our target is a wide results table, with one row for each race, and sufficient columns for any race-level covariates we want to include in the model. library(tidyverse) source(&quot;utils/util.R&quot;) load(&quot;data/relational_db.rda&quot;) load(&quot;outputs/geographies_output.rda&quot;) 3.2 Thinking ahead Let’s think about how we want to model the data. That’ll inform what columns we need. First, I decide to model only the “two-party” vote, dropping everyone who’s not a Democrat or Republican. Remember that candidates has an imputed party_replaced for candidates who sneakily ran as third-parties for an uncontested election. results_with_parties &lt;- geography_results %&gt;% inner_join( races %&gt;% filter(substr(election, 6, 6) == &quot;G&quot;) ) %&gt;% left_join(candidates_to_races) %&gt;% select(race, candidate, office, cofips:sth, vote_total, GEOID, party_replaced) ## Make sure only Dem or Rep won. If a third party ever won, we&#39;d need to rethink this... results_with_parties %&gt;% group_by(race, office, party_replaced) %&gt;% summarise(vote_total = sum(vote_total)) %&gt;% group_by(race) %&gt;% mutate(rank = rank(-vote_total)) %&gt;% filter(rank &lt;= 2) %&gt;% group_by(office, party_replaced, rank) %&gt;% count() %&gt;% spread(rank, n) ## # A tibble: 39 x 4 ## # Groups: office, party_replaced [39] ## office party_replaced `1` `2` ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 GOV DEM 3 1 ## 2 GOV REP 1 3 ## 3 STH ACT NA 1 ## 4 STH CON NA 1 ## 5 STH CST NA 3 ## 6 STH D/G NA 1 ## 7 STH DBP NA 1 ## 8 STH DEM 742 504 ## 9 STH F4B NA 1 ## 10 STH F89 NA 1 ## # ... with 29 more rows Ok, we’ve got cleaned parties. 3.3 The model Here, I’ll preview the model I use in Making the Predictions, since that dictates what data we need. How will we predict the votes? Here’s the model. Let’s call \\(sth_{yr}\\) the two-party percent of the vote for State House in year \\(y\\) in race \\(r\\). In a best case scenario, we would have polls of voters. Then we could capture simple-seeming things that our data has no idea of: How charismatic is a candidate? Are they well organized? Have they been running ads? Polls are what FiveThirtyEight uses, and why they get such good predictions. Of course, noboday actually publicly polls the 203 PA State House races. In a worst case scenario, we would have to use only data from prior elections. This would leave us completely unable to predict large swings in public sentiment, and we would have to expand our uncertainty to capture the full range of election-level random effects (aka the way that all races are correlated from year to year). Imagine trying to predict the 2018 election using only the 2016 and 2014 results, without any data from 2018 that signaled Something Is Different. We would need to produce predictions capable of saying both “maybe this year is like 2010” and “maybe this year is like 2006”. Luckily, we are somewhere in between. While we don’t have polling on this year’s State House races, we do have polling on the US Congressional races. To the extent that USC races are correlated with STH races (probably a lot), we can use the USC polls to estimate the overall tenor of the race. Better yet, we don’t have to actually use polling data itself, because FiveThirtyEight has already translated them into predicted votes. _sunglassesemoji_ Here’s the plan: model the results in a State House race as a function of past results in that district along with the US Congress results in that year: \\[ \\begin{align*} sth_{yr} =&amp; \\beta_0 + \\beta_1 sth_{y-1,r} + \\beta_2 incumbent\\_is\\_dem_{yr} + \\beta_3 sth_{y-1,r} * incumbent\\_is\\_dem_{yr} \\\\ &amp;+ \\beta_4 usc_{y,usc(r)} + \\beta_5 usc_{y,PA} + \\beta_6 uspgov_{y-1, r} + \\beta_7 uspgov_{y, PA} + \\beta_8 uspgov_{y-1, PA} + \\epsilon_{yr} \\end{align*} \\] where \\(incumbent\\_is\\_dem\\) is \\(1\\) if the democratic candidate is an incumbent, \\(-1\\) if the republican is, and \\(0\\) otherwise; \\(usc(r)\\) is the result in the entire USC district that race \\(r\\) belongs to (as opposed to just in the precinct); the subscript \\(PA\\) represents the state-wide results; and \\(uspgov\\) is the result of either the USP or the GOV race, whichever occurred that year. One thing to note is that I don’t include year-level random effects. Instead, I parametrize the vote using several annual-level covariates: this year’s USPGOV results, last year’s USPGOV results, this year’s total USC results, and an overall mean. That’s four degrees of freedom used up when we’re only going to be able to use data from seven elections; we’re already on thin ice, and hopefully uncertainty in those \\(\\beta\\)s capture annual variations. We’ll check when we model The above results will work well for races where everything was contested, but we clearly shouldn’t include uncontested races. So we’ll do two things: for races that are uncontested in year \\(y\\), we will not model them at all, since we know the running candidate will win 100% of the vote. For races that were not contested last cycle but are contested this year, we will model them entirely separately, using a different equation: Model for formerly uncontested races: \\[ \\begin{align*} sth_{yr} =&amp; \\beta_0 + \\beta_1 dem\\_is\\_uncontested_{y-1, r} + \\beta_2 dem\\_is\\_uncontested_{y-1, r} * incumbent\\_is\\_running_{y,r} \\\\ &amp;+ \\beta_4 usc_{y,usc(r)} + \\beta_5 usc_{y,PA} + \\beta_6 uspgov_{y-1, r} + \\beta_7 uspgov_{y, PA} + \\beta_8 uspgov_{y-1, PA} + \\epsilon_{yr} \\end{align*} \\] So now we know what we need: a dataframe with one row per State House race, with the last year’s STH results, and this year’s USP/GOV and USC results. First, let’s create the wide table, with the STH results. ## from here on out we only consider 2-party vote: results_with_parties &lt;- results_with_parties %&gt;% filter(party_replaced %in% c(&quot;DEM&quot;, &quot;REP&quot;)) rep_na_0 &lt;- function(x) replace(x, is.na(x), 0) sth_races &lt;- results_with_parties %&gt;% inner_join(races %&gt;% filter(office == &#39;STH&#39;)) %&gt;% group_by(race, sth, party_replaced, candidate) %&gt;% summarise(votes_sth = sum(vote_total)) %&gt;% left_join(candidates_to_races %&gt;% select(race, candidate, is_incumbent)) %&gt;% gather(&quot;key&quot;, &quot;value&quot;, votes_sth, candidate, is_incumbent) %&gt;% unite(&quot;key&quot;, party_replaced, key) %&gt;% spread(key, value, convert = TRUE) %&gt;% mutate( sth_pctdem = rep_na_0(DEM_votes_sth) / (rep_na_0(DEM_votes_sth) + rep_na_0(REP_votes_sth)) ) sth_races$incumbent_is_dem &lt;- 0 sth_races$incumbent_is_dem &lt;- with( sth_races, replace(incumbent_is_dem, !is.na(DEM_is_incumbent) &amp; as.logical(DEM_is_incumbent), 1) ) sth_races$incumbent_is_dem &lt;- with( sth_races, replace(incumbent_is_dem, !is.na(REP_is_incumbent) &amp; as.logical(REP_is_incumbent), -1) ) sth_races$dem_is_uncontested &lt;- with( sth_races, ifelse(is.na(REP_candidate), 1, ifelse(is.na(DEM_candidate), -1, 0)) ) ## We&#39;ll call our main table df df &lt;- sth_races %&gt;% mutate(year = substr(race,1,4)) head(df) ## # A tibble: 6 x 12 ## # Groups: race, sth [6] ## race sth DEM_candidate DEM_is_incumbent DEM_votes_sth REP_candidate ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;lgl&gt; &lt;int&gt; &lt;chr&gt; ## 1 2002~ 001 LINDA BEBKOJ~ NA 8895 BILL STEPHAN~ ## 2 2002~ 010 &lt;NA&gt; NA NA FRANK LAGROT~ ## 3 2002~ 100 BRUCE BEARDS~ NA 3308 GIBSON C ARM~ ## 4 2002~ 101 NOEL HUBLER ~ NA 5358 MAUREE A GIN~ ## 5 2002~ 102 DAN BACKENST~ NA 3965 PETER J ZUG ~ ## 6 2002~ 103 RON BUXTON (~ NA 7866 SHERMAN C CU~ ## # ... with 6 more variables: REP_is_incumbent &lt;lgl&gt;, REP_votes_sth &lt;dbl&gt;, ## # sth_pctdem &lt;dbl&gt;, incumbent_is_dem &lt;dbl&gt;, dem_is_uncontested &lt;dbl&gt;, ## # year &lt;chr&gt; Next, let’s get the USC results from the same year. We want to calculate the overall USC results, then apportion them to STH districts. While we could use precinct-level USC results to train the model, we don’t have precinct-level predictions from FiveThirtyEight, and will need to use only topline results for our predictions. So we’ll mimic that here. usc_results &lt;- results_with_parties %&gt;% inner_join(races %&gt;% filter(office == &#39;USC&#39;)) %&gt;% group_by(race, usc, party_replaced, candidate) %&gt;% summarise(votes_usc = sum(vote_total)) %&gt;% group_by(race) %&gt;% mutate( cand_pct = votes_usc / sum(votes_usc) ) Some of the USC races were uncontested, which will unhelpfully show up as 100% wins, and potentially skew our predictions. Let’s simplistically impute what those races would have been had they been contested, by regressing on the district’s vote in the USP/GOV race. (Typical studies find that the uncontested candidate would have won by 60%-95%.) usc_wide &lt;- usc_results %&gt;% select(race, usc, party_replaced, cand_pct) %&gt;% spread(party_replaced, cand_pct, fill=0) %&gt;% mutate( is_uncontested = ifelse( DEM == 0, &quot;REP&quot;, ifelse(REP == 0, &quot;DEM&quot;, &quot;contested&quot;) ), year = substr(race,1,4), usc_pctdem_2party = DEM / (DEM + REP), total_votes = DEM + REP ) uspgov_by_usc &lt;- results_with_parties %&gt;% filter(office %in% c(&quot;GOV&quot;, &quot;USP&quot;)) %&gt;% mutate(year = substr(race, 1, 4)) %&gt;% group_by(year, usc, party_replaced) %&gt;% summarise(vote_total = sum(vote_total)) %&gt;% spread(party_replaced, vote_total) %&gt;% mutate(uspgov_pctdem_2party = DEM / (DEM + REP)) %&gt;% select(-DEM, -REP) imputation_df &lt;- usc_wide %&gt;% left_join(uspgov_by_usc) imputation_fit &lt;- lm( usc_pctdem_2party ~ uspgov_pctdem_2party + factor(year), data=imputation_df %&gt;% filter(is_uncontested == &#39;contested&#39;) ) summary(imputation_fit) ## ## Call: ## lm(formula = usc_pctdem_2party ~ uspgov_pctdem_2party + factor(year), ## data = imputation_df %&gt;% filter(is_uncontested == &quot;contested&quot;)) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.226962 -0.049530 -0.007464 0.042506 0.296493 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.07601 0.03669 -2.072 0.04046 * ## uspgov_pctdem_2party 0.99908 0.04848 20.608 &lt; 2e-16 *** ## factor(year)2004 0.04495 0.03214 1.399 0.16454 ## factor(year)2006 0.02414 0.03003 0.804 0.42321 ## factor(year)2008 0.07660 0.02978 2.572 0.01135 * ## factor(year)2010 0.10078 0.03064 3.289 0.00132 ** ## factor(year)2012 0.05891 0.02988 1.972 0.05097 . ## factor(year)2014 -0.00992 0.03096 -0.320 0.74921 ## factor(year)2016 0.06344 0.03128 2.028 0.04479 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.0815 on 118 degrees of freedom ## Multiple R-squared: 0.7886, Adjusted R-squared: 0.7743 ## F-statistic: 55.02 on 8 and 118 DF, p-value: &lt; 2.2e-16 Let’s make sure they look sane: imputation_df$usc_pctdem_2party_imputed &lt;- ifelse( imputation_df$is_uncontested == &#39;contested&#39;, imputation_df$usc_pctdem_2party, predict(imputation_fit, newdata = imputation_df) ) ggplot(imputation_df, aes(x=uspgov_pctdem_2party, y=usc_pctdem_2party_imputed)) + geom_point(aes(color=is_uncontested)) + theme_sixtysix() Sure, they’re fine. The imputed values aren’t obviously different than the contested races. Let’s apportion the USC results to STH districts using a population-weighted average. usc_to_sth &lt;- geographies %&gt;% as.data.frame() %&gt;% group_by(vintage, sth, usc) %&gt;% summarise( pop10=sum(pop10, na.rm=TRUE) ) %&gt;% group_by(vintage, usc) %&gt;% mutate(frac_of_usc = pop10 / sum(pop10)) sth_usc_results &lt;- imputation_df %&gt;% left_join(year_to_geo_vintage) %&gt;% left_join(usc_to_sth) %&gt;% group_by(year, sth) %&gt;% summarise( usc_pctdem = weighted.mean(usc_pctdem_2party_imputed, w = total_votes * frac_of_usc) ) Join the USC results to the STH results. df &lt;- left_join(df, sth_usc_results) tail(df) ## # A tibble: 6 x 13 ## # Groups: race, sth [6] ## race sth DEM_candidate DEM_is_incumbent DEM_votes_sth REP_candidate ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;lgl&gt; &lt;int&gt; &lt;chr&gt; ## 1 2016~ 094 &lt;NA&gt; NA NA STANLEY E SA~ ## 2 2016~ 095 CAROL HILLEV~ FALSE 13726 JOEL L SEARS~ ## 3 2016~ 096 PETER MICHAE~ TRUE 17340 ROBERT F BIG~ ## 4 2016~ 097 CHARLES J KL~ FALSE 13403 STEVEN CURTI~ ## 5 2016~ 098 &lt;NA&gt; NA NA DAVID S HICK~ ## 6 2016~ 099 DUANE A GROF~ FALSE 6219 DAVID H ZIMM~ ## # ... with 7 more variables: REP_is_incumbent &lt;lgl&gt;, REP_votes_sth &lt;dbl&gt;, ## # sth_pctdem &lt;dbl&gt;, incumbent_is_dem &lt;dbl&gt;, dem_is_uncontested &lt;dbl&gt;, ## # year &lt;chr&gt;, usc_pctdem &lt;dbl&gt; Now let’s add the state-wide USP/GOV and USC races: statewide_results &lt;- results_with_parties %&gt;% inner_join( races %&gt;% filter(office %in% c(&quot;USP&quot;,&quot;GOV&quot;, &quot;USC&quot;)) %&gt;% inner_join(elections %&gt;% filter(election_type == &quot;G&quot;)) ) %&gt;% mutate(office = ifelse(office %in% c(&quot;USP&quot;, &quot;GOV&quot;), &quot;USPGOV&quot;, office)) %&gt;% group_by(election_year, party_replaced, office) %&gt;% summarise(vote_total=sum(vote_total)) %&gt;% unite(&quot;key&quot;, office, party_replaced) %&gt;% spread(key, vote_total) %&gt;% mutate( uspgov_pctdem_statewide = USPGOV_DEM / (USPGOV_DEM + USPGOV_REP), usc_pctdem_statewide = USC_DEM / (USC_DEM + USC_REP) ) df &lt;- df %&gt;% left_join( statewide_results %&gt;% select(election_year, uspgov_pctdem_statewide, usc_pctdem_statewide), by=c(&quot;year&quot;=&quot;election_year&quot;) ) head(df) ## # A tibble: 6 x 15 ## # Groups: race, sth [6] ## race sth DEM_candidate DEM_is_incumbent DEM_votes_sth REP_candidate ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;lgl&gt; &lt;int&gt; &lt;chr&gt; ## 1 2002~ 001 LINDA BEBKOJ~ NA 8895 BILL STEPHAN~ ## 2 2002~ 010 &lt;NA&gt; NA NA FRANK LAGROT~ ## 3 2002~ 100 BRUCE BEARDS~ NA 3308 GIBSON C ARM~ ## 4 2002~ 101 NOEL HUBLER ~ NA 5358 MAUREE A GIN~ ## 5 2002~ 102 DAN BACKENST~ NA 3965 PETER J ZUG ~ ## 6 2002~ 103 RON BUXTON (~ NA 7866 SHERMAN C CU~ ## # ... with 9 more variables: REP_is_incumbent &lt;lgl&gt;, REP_votes_sth &lt;dbl&gt;, ## # sth_pctdem &lt;dbl&gt;, incumbent_is_dem &lt;dbl&gt;, dem_is_uncontested &lt;dbl&gt;, ## # year &lt;chr&gt;, usc_pctdem &lt;dbl&gt;, uspgov_pctdem_statewide &lt;dbl&gt;, ## # usc_pctdem_statewide &lt;dbl&gt; Now for the hard part: adding lagged STH results. We need to crosswalk the past election results forward to the next year, using the crosswalks we made. ## first, get the results for each year sth_uspgov_results &lt;- results_with_parties %&gt;% mutate( office = replace(office, office %in% c(&quot;GOV&quot;, &quot;USP&quot;), &quot;USPGOV&quot;), year = substr(race, 1, 4) ) %&gt;% filter(office %in% c(&quot;STH&quot;, &quot;USPGOV&quot;)) %&gt;% select(year, office, cofips:sth, GEOID, party_replaced, vote_total) %&gt;% unite(&quot;key&quot;, office, party_replaced) %&gt;% spread(key, vote_total, fill=0) %&gt;% mutate( sth_is_uncontested = ifelse(STH_DEM == 0, &quot;REP&quot;, ifelse(STH_REP == 0, &#39;DEM&#39;, &#39;contested&#39;)), sth_pctdem = STH_DEM / (STH_DEM + STH_REP), uspgov_pctdem = USPGOV_DEM / (USPGOV_DEM + USPGOV_REP) ) ## now, walk them forward. results_list &lt;- list() for(year_ in seq(2002, 2016, 2)){ print(year_) vintage &lt;- year_to_geo_vintage$vintage[year_to_geo_vintage$year == year_] needed_vintage &lt;- year_to_geo_vintage$vintage[year_to_geo_vintage$year == (year_+2)] if(vintage != needed_vintage){ needed_crosswalk &lt;- paste(vintage, needed_vintage, sep=&#39;,&#39;) cw &lt;- crosswalks[[needed_crosswalk]] %&gt;% filter(pop &gt; 0 &amp; !is.na(pop)) } else { ## dummmy cw cw &lt;- data.frame( xid = unique(sth_uspgov_results$GEOID), yid = unique(sth_uspgov_results$GEOID), frac_of_x=1, frac_of_y=1 ) } geo_results_lagged &lt;- sth_uspgov_results %&gt;% filter(year == year_) %&gt;% left_join(cw, by = c(&quot;GEOID&quot; = &quot;xid&quot;)) %&gt;% group_by(yid, year) %&gt;% summarise( sth_pctdem_lagged = weighted.mean(sth_pctdem, w = frac_of_y, na.rm=TRUE), uspgov_pctdem_lagged = weighted.mean(uspgov_pctdem, w = frac_of_y, na.rm=TRUE), sth_frac_contested_lagged = weighted.mean(sth_is_uncontested == &quot;contested&quot;, w = frac_of_y, na.rm=TRUE) ) %&gt;% mutate(year = as.character(asnum(year) + 2)) %&gt;% rename(GEOID = yid) sth_results_lagged &lt;- geo_results_lagged %&gt;% left_join(year_to_geo_vintage) %&gt;% left_join(geographies %&gt;% as.data.frame %&gt;% select(-geometry)) %&gt;% group_by(year, sth) %&gt;% summarise( sth_pctdem_lagged = weighted.mean(sth_pctdem_lagged, w=pop10, na.rm=TRUE), uspgov_pctdem_lagged = weighted.mean(uspgov_pctdem_lagged, w=pop10, na.rm=TRUE), sth_frac_contested_lagged = weighted.mean(sth_frac_contested_lagged, w=pop10, na.rm=TRUE) ) results_list[[as.character(year_)]] &lt;- sth_results_lagged } ## [1] 2002 ## [1] 2004 ## [1] 2006 ## [1] 2008 ## [1] 2010 ## [1] 2012 ## [1] 2014 ## [1] 2016 sth_lagged &lt;- do.call(rbind, results_list) df &lt;- df %&gt;% left_join(sth_lagged) Finally, we need the state-wide USPGOV results from the last year. statewide_lagged &lt;- statewide_results %&gt;% group_by() %&gt;% mutate(year = as.character(asnum(election_year)+2)) %&gt;% rename( uspgov_pctdem_statewide_lagged = uspgov_pctdem_statewide, usc_pctdem_statewide_lagged = usc_pctdem_statewide ) %&gt;% select(year, uspgov_pctdem_statewide_lagged, usc_pctdem_statewide_lagged) df &lt;- left_join(df, statewide_lagged) We’re done with our training data! We have a rectangular data.frame with sufficient data for the equation above. Save it and move on. save(df, file=&quot;outputs/df.rda&quot;) 3.4 Building 2018 All that’s left is to create the corresponding table for 2018, which we will use for our predictions. We obviously don’t have the results, but we can populate the race characteristics, the lagged results, and the USC estimates from FiveThirtyEight. df_2018 &lt;- sth_lagged %&gt;% filter(year == 2018) %&gt;% left_join(statewide_lagged) %&gt;% filter(!is.na(sth)) ## fivethirtyeight&#39;s prediction for 2018 wolf is 57% df_2018$uspgov_pctdem_statewide &lt;- 0.57 ## load the race particulars cands_2018 &lt;- read_csv(&quot;data/2018_cands.csv&quot;) %&gt;% rename( d_inc = incumbent_is_dem, r_inc = incumbent_is_rep ) cands_2018 &lt;- cands_2018 %&gt;% filter(office == &#39;STH&#39;) %&gt;% mutate( sth = sprintf(&quot;%03d&quot;, asnum(district)), dem_is_uncontested = ifelse( dem_cand == &quot;No candidate&quot;, -1, ifelse(rep_cand == &quot;No candidate&quot;, 1, 0) ), incumbent_is_dem = ifelse(d_inc, 1, ifelse(r_inc, -1, 0)) ) df_2018 &lt;- df_2018 %&gt;% left_join( cands_2018 %&gt;% select(sth, dem_is_uncontested, incumbent_is_dem) ) ## now we need the state predictions fivethirtyeight &lt;- read_csv(&quot;data/congress_races_2018_538.csv&quot;) %&gt;% mutate(usc = sprintf(&quot;%02d&quot;, district)) %&gt;% rename(usc_pctdem = fivethirtyeight) ## notice that one race is uncontested. we need to impute it. ## it looks like uncontested winners should be imputed as 66%: print( imputation_df %&gt;% group_by(is_uncontested) %&gt;% summarise(mean_imputed = mean(usc_pctdem_2party_imputed)) ) ## # A tibble: 3 x 2 ## is_uncontested mean_imputed ## &lt;chr&gt; &lt;dbl&gt; ## 1 contested 0.501 ## 2 DEM 0.665 ## 3 REP 0.344 fivethirtyeight$usc_pctdem[fivethirtyeight$district == &#39;18&#39;] &lt;- 0.66 df_2018$usc_pctdem_statewide &lt;- mean(fivethirtyeight$usc_pctdem) sth_usc_538 &lt;- geographies %&gt;% as.data.frame() %&gt;% select(-geometry) %&gt;% filter(vintage == 2018 &amp; !is.na(pop10)) %&gt;% left_join( fivethirtyeight %&gt;% select(usc, usc_pctdem) ) %&gt;% group_by(sth) %&gt;% summarise( usc_pctdem = weighted.mean(usc_pctdem, w = pop10, na.rm = TRUE) ) df_2018 &lt;- left_join(df_2018, sth_usc_538) df_2018$race &lt;- paste0(&quot;2018 G STH STH-&quot;, df_2018$sth) head(df_2018) ## # A tibble: 6 x 13 ## # Groups: year [1] ## year sth sth_pctdem_lagg~ uspgov_pctdem_l~ sth_frac_contes~ ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2018 001 0.736 0.661 1 ## 2 2018 002 0.646 0.580 1 ## 3 2018 003 0.597 0.480 1 ## 4 2018 004 0 0.362 0 ## 5 2018 005 0 0.321 0 ## 6 2018 006 0.400 0.351 1 ## # ... with 8 more variables: uspgov_pctdem_statewide_lagged &lt;dbl&gt;, ## # usc_pctdem_statewide_lagged &lt;dbl&gt;, uspgov_pctdem_statewide &lt;dbl&gt;, ## # dem_is_uncontested &lt;dbl&gt;, incumbent_is_dem &lt;dbl&gt;, ## # usc_pctdem_statewide &lt;dbl&gt;, usc_pctdem &lt;dbl&gt;, race &lt;chr&gt; ## I think we&#39;re done save(df_2018, file=&quot;outputs/df_2018.rda&quot;) So that’s it! We’ve got our data. Now for the modeling. "],
["predicting-the-state-house.html", "Chapter 4 Predicting the State House 4.1 The Pennsylvania State House 4.2 Overview of the Data 4.3 Our model 4.4 Fitting the model 4.5 Results for all years 4.6 Predicting 2018 4.7 Fast forward to today", " Chapter 4 Predicting the State House library(tidyverse) source(&quot;utils/util.R&quot;) set.seed(215) ## df.rda was prepped in 03_rectangular_data.Rmd vote_df &lt;- safe_load(&quot;outputs/df.rda&quot;) vote_df &lt;- vote_df %&gt;% group_by() 4.1 The Pennsylvania State House The Pennsylvania General Assembly is divided into two houses, the Upper and Lower. The Upper (aka the State Senate) has 50 senators, each of whom serve four-year terms. The Lower (aka the State House) has 203 representatives who serve two-year terms. We’re focused on the second (the Senate was certainly out of play in 2018, since only 25 seats are open in a given year). Control of the State House has swung sharply. Democrats eked out a 102-seat majority in 2006, but quickly lost it in 2010, and Republicans have dominated since. The effect of gerrymandering are apparent in the 2012-to-2014 shift (remember that due to a court challenge, the old districts were used in 2012). party_colors &lt;- c(Dem = strong_blue, Rep = strong_red) seats_won_by_party &lt;- vote_df %&gt;% group_by(year) %&gt;% summarise( Dem = sum(sth_pctdem &gt; 0.5), Rep = sum(sth_pctdem &lt; 0.5) ) %&gt;% group_by() %&gt;% gather(key=&quot;party&quot;, value=&quot;n_seats&quot;, Dem:Rep) ggplot( seats_won_by_party, aes(x = asnum(year), y = n_seats, color = party) ) + geom_hline(yintercept=101.5, color = &quot;grey30&quot;) + geom_point(size = 4) + geom_line(aes(group = party), size = 1) + scale_color_manual(values=party_colors, guide=FALSE) + theme_sixtysix() + annotate( &quot;text&quot;, label = c(&quot;Dem&quot;, &quot;Rep&quot;), x = 2015, y=c(86, 117), color=c(strong_blue, strong_red), fontface=&quot;bold&quot; ) + scale_x_continuous(&#39;&#39;, breaks = seq(2002,2016,2)) + scale_y_continuous(&quot;Number of Seats won&quot;) + ggtitle(&quot;Pennsylvania House Party Control&quot;) In 2016, Republicans increased their dominance of the house to 39 seats. library(sf) library(rgeos) sth_11 &lt;- st_read( &quot;data/state_house/tigris_lower_house_2011.shp&quot;, quiet=TRUE ) %&gt;% mutate(vintage=&quot;&lt;=2012&quot;) sth_15 &lt;- st_read( &quot;data/state_house/tigris_lower_house_2015.shp&quot;, quiet=TRUE ) %&gt;% mutate(vintage=&quot;&gt;2012&quot;) sth_sf &lt;- rbind(sth_11, sth_15) %&gt;% rename(sth = SLDLST) sth_pts &lt;- mapply(st_centroid, sth_sf$geometry) %&gt;% t sth_sf &lt;- sth_sf %&gt;% mutate(x=sth_pts[,1], y=sth_pts[,2]) pa_union &lt;- st_read(&quot;data/congress/tigris_usc_2015.shp&quot;) %&gt;% st_union() %&gt;% st_transform(st_crs(sth_15)) ## Reading layer `tigris_usc_2015&#39; from data source `\\\\jove.design.upenn.edu\\home\\staff-admin\\PRAX\\sydng\\My Documents\\UrbanSpatial\\sixtysixwards_musa_workshop\\ElectionAnalyticsWorkshop\\data\\congress\\tigris_usc_2015.shp&#39; using driver `ESRI Shapefile&#39; ## Simple feature collection with 18 features and 12 fields ## geometry type: POLYGON ## dimension: XY ## bbox: xmin: 1189586 ymin: 140905.3 xmax: 2814854 ymax: 1165942 ## epsg (SRID): NA ## proj4string: +proj=lcc +lat_1=40.96666666666667 +lat_2=39.93333333333333 +lat_0=39.33333333333334 +lon_0=-77.75 +x_0=600000.0000000001 +y_0=0 +datum=NAD83 +units=us-ft +no_defs sth_vintage &lt;- data.frame( year = seq(2002, 2018, 2) ) %&gt;% mutate(vintage = ifelse(year &lt;= 2012, &quot;&lt;=2012&quot;, &quot;&gt;2012&quot;)) ggplot( sth_sf %&gt;% filter(vintage == &quot;&gt;2012&quot;) %&gt;% left_join( vote_df %&gt;% filter(year == 2016) ) ) + geom_sf(fill=&quot;white&quot;) + geom_point( aes( x=x, y=y, color = (sth_pctdem &gt; 0.5), alpha = abs(sth_pctdem - 0.5) ), pch=16, size=4 ) + scale_color_manual( values = c(&quot;TRUE&quot;=strong_blue, &quot;FALSE&quot;=strong_red), guide=FALSE ) + scale_alpha_continuous(range=c(0,0.5), guide=FALSE) + theme_map_sixtysix() + ggtitle(&quot;State House 2016 Results&quot;) They did so with a ton of races between 55-70% Republican, the kind of races that might be in play in a D+9 election FiveThirtyEight was predicting. ggplot( vote_df %&gt;% filter(year == 2016), aes(x=sth_pctdem, fill=(sth_pctdem &gt; 0.5)) ) + geom_histogram(boundary = 0.5, binwidth = 0.05) + scale_fill_manual( values=c(&quot;TRUE&quot;=strong_blue, &quot;FALSE&quot;=strong_red), guide=FALSE ) + theme_sixtysix() + ggtitle(&quot;State House 2016 results&quot;) Flashback to October 2018. As we enter the 2018 election, it’s nationally clear that Democrats are going to have a strong election. But how strong? And how will that translate down to these State Races? Simply put: do Democrats have a shot to take back the PA House? In this post we’ll predict the outcomes of the race. 4.2 Overview of the Data The main dataframe: vote_df: a data.frame with a row for each race. It includes lagged variables from prior elections, cross-walked where necessary. vote_df %&gt;% filter(year == 2016) %&gt;% head %&gt;% as.data.frame() ## race sth DEM_candidate DEM_is_incumbent ## 1 2016 G STH STH-1 001 PATRICK J HARKINS (STH) TRUE ## 2 2016 G STH STH-10 010 JARET A GIBBONS (STH) TRUE ## 3 2016 G STH STH-100 100 DALE ALLEN HAMBY (STH) FALSE ## 4 2016 G STH STH-101 101 LORRAINE HELEN SCUDDER (STH) FALSE ## 5 2016 G STH STH-102 102 JACOB H LONG (STH) FALSE ## 6 2016 G STH STH-103 103 PATTY H KIM (STH) TRUE ## DEM_votes_sth REP_candidate REP_is_incumbent ## 1 14785 WILLIAM CROTTY (STH) FALSE ## 2 11224 AARON JOSEPH BERNSTINE (STH) FALSE ## 3 6140 BRYAN D CUTLER (STH) TRUE ## 4 9752 FRANCIS XAVIER RYAN (STH) FALSE ## 5 8549 RUSSELL H DIAMOND (STH) TRUE ## 6 20846 &lt;NA&gt; NA ## REP_votes_sth sth_pctdem incumbent_is_dem dem_is_uncontested year ## 1 5491 0.7291872 1 0 2016 ## 2 15807 0.4152270 1 0 2016 ## 3 17416 0.2606555 -1 0 2016 ## 4 19800 0.3299946 0 0 2016 ## 5 19858 0.3009469 -1 0 2016 ## 6 NA 1.0000000 1 1 2016 ## usc_pctdem uspgov_pctdem_statewide usc_pctdem_statewide ## 1 0.3487901 0.4962162 0.4588045 ## 2 0.3736409 0.4962162 0.4588045 ## 3 0.4334167 0.4962162 0.4588045 ## 4 0.4093855 0.4962162 0.4588045 ## 5 0.4100933 0.4962162 0.4588045 ## 6 0.3459720 0.4962162 0.4588045 ## sth_pctdem_lagged uspgov_pctdem_lagged sth_frac_contested_lagged ## 1 1.0000000 0.7252871 0 ## 2 1.0000000 0.5103014 0 ## 3 0.0000000 0.3410035 0 ## 4 0.2932679 0.4109548 1 ## 5 0.3671351 0.3575663 1 ## 6 1.0000000 0.7775791 0 ## uspgov_pctdem_statewide_lagged usc_pctdem_statewide_lagged ## 1 0.5493505 0.4446998 ## 2 0.5493505 0.4446998 ## 3 0.5493505 0.4446998 ## 4 0.5493505 0.4446998 ## 5 0.5493505 0.4446998 ## 6 0.5493505 0.4446998 4.2.1 Exercise 1: Fit a Model Pause here, and fit a model to predict sth_pctdem using vote_df. 4.3 Our model NOTE: This is different from the model I actually used to make my predictions. I’ve cleaned up the data significantly, and improved the method using my lessons learned. It’s what I wish I’d done. Let’s call \\(sth_{yr}\\) the two-party percent of the vote for the Democrat for State House in year \\(y\\) in race \\(r\\). 4.3.1 Using contemporaneous data To predict an election, there are obviously useful covariates: historical results in the district, incumbency, four-year cycles. But the easy data leaves a gaping hole: all of the races can swing together from election to election. That is, years have large “random effects”. In a best case scenario, we would have polls of voters on the State House races. Then we could capture simple-seeming things that our data has no idea of: How charismatic is a candidate? Are they well organized? Have they been running ads? Polls are what FiveThirtyEight uses, and why they get such good predictions. Of course, noboday actually publicly polls the 203 PA State House races. In a worst case scenario, we would have to use only data from prior elections. This would leave us completely unable to predict large swings in public sentiment, and we would have to expand our uncertainty to capture the full range of election-level random effects (aka the way that all races are correlated from year to year). Imagine trying to predict the 2018 election using only the 2016 and 2014 results, without any data from 2018 that signaled Something Is Different. We would need to produce predictions capable of saying both “maybe this year is like 2010” and “maybe this year is like 2006”. Luckily, we are somewhere in between. While we don’t have polling on this year’s State House races, we do have polling on the US Congressional races. To the extent that USC races are correlated with STH races (probably a lot), we can use the USC polls to estimate the overall tenor of the race. Better yet, we don’t have to actually use polling data itself, because FiveThirtyEight has already translated them into predicted votes. 4.3.2 The model Here’s the plan: model the results in a State House race as a function of past results in that district along with the US Congress results in that year. contested_form &lt;- sth_pctdem ~ 1 + sth_pctdem_lagged + incumbent_is_dem + incumbent_is_running:sth_pctdem_lagged + usc_pctdem + usc_pctdem_statewide + I(uspgov_pctdem_lagged - uspgov_pctdem_statewide_lagged) where incumbent_is_dem is \\(1\\) if the democratic candidate is an incumbent, \\(-1\\) if the republican is, and \\(0\\) otherwise; usc_pct_dem is the result in the entire USC district that the race belongs to (as opposed to just in the precinct); the usc_pctdem_statewide is the state-wide USC result (weighted by votes); and uspgov_pctdem is the result of either the USP or the GOV race, whichever occurred that year. One thing to note is that I don’t include year-level random effects in this specification. But we’ll be careful about year-level effects later. We only have a few years in the dataset (we’ll be able to use 7), so we don’t want to use too many year-level covariates (I have two: the intercept and the statewide vote for USC). On the other hand, we have 203 races per year, so we can include a ton of covariates that vary across races: incumbency, past results, USC results for specific districts. The above model will work well for races where everything was contested, but we clearly shouldn’t include uncontested races: we already know what those results will be, and the historical 100%s could mess up our other estimates. So we’ll do two things: for races that are uncontested in year \\(y\\), we will not model them at all, since we know the running candidate will win 100% of the vote. For races that were not contested last cycle but are contested this year, we will model them entirely separately, using a different equation: Model for formerly uncontested races: uncontested_form &lt;- sth_pctdem ~ 1 + dem_won_lagged + incumbent_is_running + dem_won_lagged:incumbent_is_running + usc_pctdem + usc_pctdem_statewide + I(uspgov_pctdem_lagged - uspgov_pctdem_statewide_lagged) 4.3.3 Bootstrapping To generate uncertainty and cross-validate, we’ll bootstrap. We won’t naively do row-level bootstrapping. I’m seriously worried that there could be systematic swings from election to election (aka election-level random effects or interactions). By only bootstrapping the rows, we would be cheating then: we would actually always have observations from every election, and never get real uncertainty of election-level randomness. Instead, we want the Multiway Bootstrap. We’ll bootstrap at the year-level where we think effects are plausibly independent. Notice that this takes our N down from 203 * 7 races down to 7 years. Oof. Here’s my approach: I’ll fit the model by bootstrapping years. In order to validate, we’ll walk through holding out each election as our test set. We only have seven elections to do this for (2004 - 2016; we can’t use 2002 since we need lagged data), but hopefully that’s enough to spot bad behaviors. This isn’t the best practice of never touching a test set, but when N=7 you can’t do best practices. Instead, I claim I’ve chosen a super conservative approach, and we’ll need to be honest about not overfitting the data. 4.3.4 Crosswalking Before we get to fitting the model, an aside. Remember that some of the lagged data required crosswalking. Thus, some of the lagged historic races may be fractionally contested: some of the area may have had uncontested races, and others contested. hist(vote_df$sth_frac_contested_lagged) I’m going to arbitrarily call you uncontested if half of the population or more lived in races that were uncontested last cycle. _shrugemoji_ Ok, fit the model. 4.4 Fitting the model We’ll build up our helper functions to fit the model. First, we’ll write fit_once, which fits a single model for a single year. Then we’ll write bootstrap, which bootstraps years of the data.frame, and runs fit_once many times, generating bootstrapped predictions for a single holdout year. Finally, we’ll loop through each year as a holdout, getting bootstraped results for each. Let’s create an S4 class to hold our results. The result of a single model fit will return the following: - Integer holdout_year. - A data.frame with the predicted mean outcome of STH; columns sth, sth_pctdem, pred. - A data.frame with the predictions for the test set plus random noise of the appropriate size, to serve in our full prediction interval. - A tidy dataframe with the coefficients of the regression fits. - A dataframe with generic summary stats we can add later setClass( &quot;FitResult&quot;, representation( holdout_year = &quot;integer&quot;, pred = &quot;data.frame&quot;, test_sample = &quot;data.frame&quot;, coefs = &quot;data.frame&quot;, summary_stats = &quot;data.frame&quot; ) ) We’ll also create an S4 class called Condition, which contains how to split up the data into models (for us, “contested”, “uncontested”, and “previously_uncontested”). setClass( &quot;Condition&quot;, slots=c( name=&quot;character&quot;, formula=&quot;formula&quot;, filter_func=&quot;function&quot;, ## a function of df that returns vector of TRUE/FALSE warn=&quot;logical&quot; ## should the model warn if the fit raises a warning? ) ) conditions &lt;- c( new( &quot;Condition&quot;, name=&quot;uncontested&quot;, formula=sth_pctdem ~ sign(dem_is_uncontested), filter_func=function(df) abs(df$dem_is_uncontested) == 1, warn=FALSE ), new( &quot;Condition&quot;, name=&quot;previously_uncontested&quot;, formula=uncontested_form, filter_func=function(df) { abs(df$dem_is_uncontested) != 1 &amp; df$sth_frac_contested_lagged &lt;= 0.5 }, warn=TRUE ), new( &quot;Condition&quot;, name=&quot;contested&quot;, formula=contested_form, filter_func=function(df) { abs(df$dem_is_uncontested) != 1 &amp; df$sth_frac_contested_lagged &gt; 0.5 }, warn=TRUE ) ) add_cols &lt;- function(df0){ df0$dem_won_lagged &lt;- df0$sth_pctdem_lagged &gt; 0.5 df0$incumbent_is_running &lt;- abs(df0$incumbent_is_dem) df0$uspgov_is_usp &lt;- (asnum(df0$year) %% 4) == 0 df0 } vote_df &lt;- vote_df %&gt;% add_cols get_holdout_set &lt;- function(df0, holdout_year){ ifelse(df0$year %in% holdout_year, &quot;test&quot;, &quot;train&quot;) } add_condition &lt;- function(df0, conditions){ df0$condition &lt;- NA for(condition in conditions){ if(!is(condition, &quot;Condition&quot;)) stop(&quot;condition is not of class Condition&quot;) df0$condition[condition@filter_func(df0)] &lt;- condition@name } if(any(is.na(df0$condition))) warning(&quot;the conditions don&#39;t cover the full dataset&quot;) return(df0) } ## can&#39;t use 2002 since it doesn&#39;t have lagged data vote_df &lt;- vote_df %&gt;% filter(year &gt; 2002) vote_df &lt;- vote_df %&gt;% add_condition(conditions) Let’s define fit_once: fit_once &lt;- function(df0, holdout_year, conditions, verbose = TRUE){ df0$set &lt;- get_holdout_set(df0, holdout_year) df0$pred &lt;- NA coef_results &lt;- list() sd_err &lt;- list() for(cond in conditions){ if(!cond@warn) wrapper &lt;- suppressWarnings else wrapper &lt;- identity fit &lt;- wrapper( lm( cond@formula, data = df0 %&gt;% filter(condition == cond@name &amp; set == &quot;train&quot;) ) ) sd_err[cond@name] &lt;- sd(fit$residuals) coef_results[[cond@name]] &lt;- broom::tidy(fit) %&gt;% mutate(condition=cond@name) df0$pred[df0$condition == cond@name] &lt;- predict( fit, newdata = df0[df0$condition == cond@name,] ) if(verbose) { print(sprintf(&quot;%s Model&quot;, cond@name)) print(wrapper(summary(fit))) } } ## estimate sd of year random effects year_re &lt;- df0 %&gt;% filter(condition != &quot;uncontested&quot; &amp; set == &quot;train&quot;) %&gt;% group_by(year) %&gt;% summarise(re = mean(sth_pctdem - pred)) year_sd &lt;- sd(year_re$re) resid_sd &lt;- sapply( sd_err[names(sd_err) != &quot;uncontested&quot;], function(e) sqrt(e^2 - year_sd^2) ) summary_stat_df &lt;- data.frame( stat = c(&quot;year_sd&quot;, paste0(&quot;resid_sd_&quot;, names(resid_sd))), value = c(year_sd, unlist(resid_sd)) ) ## this is a noisy estimate, where we generate a sample with year effect and residual noise sample &lt;- df0 %&gt;% filter(set == &#39;test&#39;) %&gt;% mutate( pred_samp = pred + ifelse( condition == &quot;uncontested&quot;, 0, rnorm(n(), mean=0, sd=resid_sd[condition]) + rnorm(1, mean=0, sd=year_sd) ) ) %&gt;% select(race, sth, condition, pred, pred_samp) return(new( &quot;FitResult&quot;, pred=df0[,c(&quot;sth&quot;, &quot;year&quot;, &quot;pred&quot;)], coefs=do.call(rbind, coef_results), test_sample=sample, holdout_year=as.integer(holdout_year), summary_stats=summary_stat_df )) } results_once &lt;- fit_once( vote_df, 2016, conditions ) ## [1] &quot;uncontested Model&quot; ## ## Call: ## lm(formula = cond@formula, data = df0 %&gt;% filter(condition == ## cond@name &amp; set == &quot;train&quot;)) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5.980e-16 -5.980e-16 0.000e+00 0.000e+00 1.103e-13 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 5.000e-01 2.207e-16 2.265e+15 &lt;2e-16 *** ## sign(dem_is_uncontested) 5.000e-01 2.207e-16 2.265e+15 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 5.279e-15 on 570 degrees of freedom ## Multiple R-squared: 1, Adjusted R-squared: 1 ## F-statistic: 5.132e+30 on 1 and 570 DF, p-value: &lt; 2.2e-16 ## ## [1] &quot;previously_uncontested Model&quot; ## ## Call: ## lm(formula = cond@formula, data = df0 %&gt;% filter(condition == ## cond@name &amp; set == &quot;train&quot;)) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.212129 -0.033724 0.001324 0.046809 0.181318 ## ## Coefficients: ## Estimate ## (Intercept) 0.19135 ## dem_won_laggedTRUE 0.08741 ## incumbent_is_running -0.06921 ## usc_pctdem 0.17552 ## usc_pctdem_statewide 0.38672 ## I(uspgov_pctdem_lagged - uspgov_pctdem_statewide_lagged) 0.49349 ## dem_won_laggedTRUE:incumbent_is_running 0.12429 ## Std. Error ## (Intercept) 0.05758 ## dem_won_laggedTRUE 0.02075 ## incumbent_is_running 0.01417 ## usc_pctdem 0.05105 ## usc_pctdem_statewide 0.11606 ## I(uspgov_pctdem_lagged - uspgov_pctdem_statewide_lagged) 0.05359 ## dem_won_laggedTRUE:incumbent_is_running 0.02236 ## t value Pr(&gt;|t|) ## (Intercept) 3.323 0.001062 ## dem_won_laggedTRUE 4.213 3.84e-05 ## incumbent_is_running -4.886 2.14e-06 ## usc_pctdem 3.438 0.000716 ## usc_pctdem_statewide 3.332 0.001030 ## I(uspgov_pctdem_lagged - uspgov_pctdem_statewide_lagged) 9.208 &lt; 2e-16 ## dem_won_laggedTRUE:incumbent_is_running 5.559 8.80e-08 ## ## (Intercept) ** ## dem_won_laggedTRUE *** ## incumbent_is_running *** ## usc_pctdem *** ## usc_pctdem_statewide ** ## I(uspgov_pctdem_lagged - uspgov_pctdem_statewide_lagged) *** ## dem_won_laggedTRUE:incumbent_is_running *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.06574 on 196 degrees of freedom ## Multiple R-squared: 0.871, Adjusted R-squared: 0.8671 ## F-statistic: 220.6 on 6 and 196 DF, p-value: &lt; 2.2e-16 ## ## [1] &quot;contested Model&quot; ## ## Call: ## lm(formula = cond@formula, data = df0 %&gt;% filter(condition == ## cond@name &amp; set == &quot;train&quot;)) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.171925 -0.040997 0.000342 0.038763 0.184274 ## ## Coefficients: ## Estimate ## (Intercept) 0.088905 ## sth_pctdem_lagged 0.488134 ## incumbent_is_dem 0.059975 ## usc_pctdem 0.021814 ## usc_pctdem_statewide 0.325711 ## I(uspgov_pctdem_lagged - uspgov_pctdem_statewide_lagged) 0.330569 ## sth_pctdem_lagged:incumbent_is_running -0.011987 ## Std. Error ## (Intercept) 0.038633 ## sth_pctdem_lagged 0.032938 ## incumbent_is_dem 0.004633 ## usc_pctdem 0.030635 ## usc_pctdem_statewide 0.073762 ## I(uspgov_pctdem_lagged - uspgov_pctdem_statewide_lagged) 0.035356 ## sth_pctdem_lagged:incumbent_is_running 0.014148 ## t value Pr(&gt;|t|) ## (Intercept) 2.301 0.0218 ## sth_pctdem_lagged 14.820 &lt; 2e-16 ## incumbent_is_dem 12.945 &lt; 2e-16 ## usc_pctdem 0.712 0.4768 ## usc_pctdem_statewide 4.416 1.27e-05 ## I(uspgov_pctdem_lagged - uspgov_pctdem_statewide_lagged) 9.350 &lt; 2e-16 ## sth_pctdem_lagged:incumbent_is_running -0.847 0.3973 ## ## (Intercept) * ## sth_pctdem_lagged *** ## incumbent_is_dem *** ## usc_pctdem ## usc_pctdem_statewide *** ## I(uspgov_pctdem_lagged - uspgov_pctdem_statewide_lagged) *** ## sth_pctdem_lagged:incumbent_is_running ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.05923 on 436 degrees of freedom ## Multiple R-squared: 0.8763, Adjusted R-squared: 0.8746 ## F-statistic: 514.9 on 6 and 436 DF, p-value: &lt; 2.2e-16 We’ve fit the model once, using 2016 as the holdout. Let’s examine the results. First, make sure the coefficients of the models make sense to you. They’re purposefully simple models, where we just estimate the correlation of State House results a bunch of other races, past and present. Contested Model: All of the obvious culprits have positive correlations: the lagged STH results, incumbent party, USC results statewide, and lagged USP/GOV results from the district (relative to statewide). Interestingly, the interaction is insignificant: knowing that the incumbent is running doesn’t make the last election’s results any more pertinent. Uncontested Model: Similarly, these results are what we expected. Notice that the coefficient of incumbent_is_running means that a Republican incumbent gets a 6.9 point boost, whereas incumbent_is_running + dem_won_laggedTRUE:incumbent_is_running means a Democratic incumbent gets a 5.5 point boost, so both are positive. Second, let’s plot the prediction. supplement_pred &lt;- function(pred, holdout_year, df0=vote_df){ pred_supplemented &lt;- pred %&gt;% left_join( df0 %&gt;% mutate(set = get_holdout_set(., holdout_year)) %&gt;% select(sth, year, race, condition, set, sth_pctdem), by = c(&quot;sth&quot;, &quot;year&quot;) ) if(!nrow(pred) == nrow(pred_supplemented)) stop(&quot;rows were duplicated&quot;) return(pred_supplemented) } supplement_pred_from_fit &lt;- function(fr, df0=vote_df){ supplement_pred(fr@pred, fr@holdout_year, df0) } prediction_plot &lt;- function(fr, condition_name){ if(!is(fr, &quot;FitResult&quot;)) stop(&quot;fr must be of type FitResult&quot;) if(!condition_name %in% unique(fr@coefs$condition)) { stop(&quot;condition doesn&#39;t match available conditions&quot;) } ggplot( fr %&gt;% supplement_pred_from_fit() %&gt;% filter(condition == condition_name), aes(x = pred, y = sth_pctdem) ) + geom_point() + geom_abline(slope=1, intercept=0) + geom_hline(yintercept = 0.5, color = &quot;grey50&quot;)+ geom_vline(xintercept = 0.5, color = &quot;grey50&quot;)+ facet_grid(~set) + coord_fixed() + ggtitle(sprintf(&quot;Predicted values of %s model, %s&quot;, condition_name, fr@holdout_year)) } prediction_plot(results_once, &quot;contested&quot;) prediction_plot(results_once, &quot;previously_uncontested&quot;) prediction_plot(results_once, &quot;uncontested&quot;) Looks ok. Now, let’s bootstrap the races, still sticking to a single year. setClass( ## BootstrapResult is like FitResult but dfs will have column `sim` &quot;BootstrapResult&quot;, slots=c( nboot=&quot;numeric&quot; ), contains=&quot;FitResult&quot; ) rbind_slots &lt;- function(obj_list, result_slot){ bind_rows( lapply(obj_list, slot, result_slot), .id = &quot;sim&quot; ) } construct_bsresult &lt;- function(fitresult_list){ nboot &lt;- length(fitresult_list) holdout_year &lt;- unique(sapply(fitresult_list, slot, &quot;holdout_year&quot;)) bs &lt;- new(&quot;BootstrapResult&quot;, nboot=nboot, holdout_year=as.integer(holdout_year)) for(sl in c(&quot;pred&quot;,&quot;coefs&quot;,&quot;test_sample&quot;)){ slot(bs, sl) &lt;- rbind_slots(fitresult_list, sl) } return(bs) } bootstrap &lt;- function(df0, holdout_year, conditions, nboot=500, verbose=TRUE, ...){ fitresult_list &lt;- list() years &lt;- as.character(seq(2004, 2016, 2)) train_years &lt;- years[!years %in% holdout_year] n_train_years &lt;- length(years) df0 &lt;- df0 %&gt;% mutate(year = as.character(year)) for(i in 1:nboot){ bs_year_samp &lt;- data.frame(year = sample(train_years, replace=TRUE)) bsdf &lt;- rbind( bs_year_samp %&gt;% left_join(df0, by = &quot;year&quot;), df0 %&gt;% filter(get_holdout_set(., holdout_year) == &quot;test&quot;) ) fitresult_list[[i]] &lt;- fit_once(bsdf, holdout_year, conditions, verbose=FALSE, ...) if(verbose &amp; (i %% 100 == 0)) print(i) } return( construct_bsresult(fitresult_list) ) } bs &lt;- bootstrap( vote_df, 2016, conditions, verbose=TRUE ) ## [1] 100 ## [1] 200 ## [1] 300 ## [1] 400 ## [1] 500 Let’s see how we did. First, do the coefficients make sense? coef_plot &lt;- function(bs, condition_name=&#39;contested&#39;){ coef_df &lt;- bs@coefs %&gt;% filter(condition == condition_name) ggplot( coef_df, aes(x=term, y=estimate) ) + geom_boxplot(outlier.color = NA) + theme_sixtysix() + theme(axis.text.x = element_text(angle = 90, hjust=1, vjust=0.5)) + ylim(-1,1)+ ggtitle( paste(&quot;Coefficients of&quot;, condition_name), paste(&quot;Holdout:&quot;, bs@holdout_year) ) } coef_plot(bs, &#39;contested&#39;) coef_plot(bs, &#39;previously_uncontested&#39;) coef_plot(bs, &#39;uncontested&#39;) Sure. How did the predictions perform? Let’s look at individual races. get_pred_with_true_results &lt;- function(bs){ true_results &lt;- bs@pred %&gt;% filter(year == bs@holdout_year) %&gt;% supplement_pred(bs@holdout_year) true_results } gg_race_pred &lt;- function(bs){ holdout_year &lt;- bs@holdout_year true_results &lt;- get_pred_with_true_results(bs) race_order &lt;- bs@test_sample %&gt;% group_by(race) %&gt;% summarise(m = mean(pred_samp)) %&gt;% with(race[order(m)]) ggplot( bs@test_sample %&gt;% mutate(race = factor(race, levels = race_order)), aes(x=race, y=pred_samp) ) + geom_hline(yintercept=0.5, size=1, color = &#39;grey30&#39;)+ geom_boxplot(outlier.colour = NA, alpha = 0.5)+ geom_point( data = true_results, aes(y=sth_pctdem), color=&quot;blue&quot; ) + theme_sixtysix()+ theme( panel.grid.major.x = element_blank(), axis.text.x = element_blank() ) + xlab(&quot;race (sorted by predicted pct dem)&quot;) + scale_y_continuous(&quot;pct dem&quot;, breaks = seq(0,1,0.25))+ ggtitle(paste(&quot;Race-by-race predictions for&quot;, holdout_year), &quot;Blue is actual results.&quot;) } gg_race_pred(bs) Looks ok. Nothing insane. 4.4.1 Exercise 2: Make a diagnostic plot Pause here, and make your own diagnostic plot of the results. 4.4.2 More diagnostics Another way to look at the results is as a scatterplot of predicted versus actual results (we’ll only do this for the holdout). gg_race_scatter &lt;- function(bs){ holdout_year &lt;- bs@holdout_year true_results &lt;- get_pred_with_true_results(bs) ggplot( bs@test_sample %&gt;% group_by(race, sth, condition) %&gt;% summarise( ymean = mean(pred_samp), ymin = quantile(pred_samp, 0.025), ymax = quantile(pred_samp, 0.975) ) %&gt;% left_join(true_results), aes(x=sth_pctdem, y=ymean) ) + geom_abline(slope=1, size=1, color = &#39;grey30&#39;)+ geom_linerange( aes(ymin=ymin, ymax=ymax, group=race) )+ geom_point( aes(y=ymean, color=condition) ) + theme_sixtysix()+ xlab(&quot;True Result&quot;) + scale_y_continuous(&quot;Prediction&quot;, breaks = seq(0,1,0.25))+ coord_fixed() + ggtitle(&quot;Race prediction intervals&quot;, holdout_year) } gg_race_scatter(bs) How about the percentiles of the true results with respect to the predicted distribution? These should be uniform. gg_resid &lt;- function(bs){ holdout_year &lt;- bs@holdout_year true_results &lt;- get_pred_with_true_results(bs) pctl &lt;- bs@test_sample %&gt;% filter(condition != &quot;uncontested&quot;) %&gt;% left_join(true_results) %&gt;% group_by(race, sth, condition) %&gt;% summarise(pctl = mean(sth_pctdem &gt; pred_samp)) ggplot(pctl, aes(x=pctl, fill = condition)) + geom_histogram(binwidth = 0.05) + scale_fill_manual( values = c( contested = strong_green, previously_uncontested = strong_purple ) ) + theme_sixtysix() } gg_resid(bs) Maybe they’re uniform. They’re not obviously bad (you know it when they’re obviously bad). What if we map them? map_residuals &lt;- function(bs, cond_name, df0=vote_df){ holdout_year &lt;- bs@holdout_year sf_vintage &lt;- sth_vintage %&gt;% filter(year==holdout_year) %&gt;% with(vintage) sf &lt;- sth_sf %&gt;% filter(vintage == sf_vintage) bs_pred &lt;- bs@test_sample %&gt;% group_by(sth, race, condition) %&gt;% summarise( pred = mean(pred) ) %&gt;% left_join(df0 %&gt;% select(race, sth_pctdem)) %&gt;% mutate(resid = sth_pctdem - pred) ggplot( sf %&gt;% left_join(bs_pred %&gt;% filter(condition == cond_name)) ) + geom_sf(data = pa_union) + geom_point( aes(x=x, y=y, size=abs(resid), color=ifelse(resid &lt; 0, &quot;Dem&quot;, &quot;Rep&quot;)) ) + scale_color_manual( values=c(Dem = strong_blue, Rep = strong_red), guide=FALSE ) + scale_size_area(guide = FALSE) + theme_map_sixtysix() + ggtitle( paste(&quot;Residual Map for&quot;, holdout_year,&quot;,&quot;, cond_name,&quot;model&quot;), &quot;Blue means Dem overpredicted, Red means Rep&quot; ) } map_residuals(bs, &quot;contested&quot;) map_residuals(bs, &quot;previously_uncontested&quot;) There’s a story here. Looks like we are systematically over-predicting the Dems in Philadelphia and Pittsburgh, and over-predicting Republicans in the immediate suburbs. We won’t raise the alarm yet, and will check out the results across years. Finally, what was our topline number? pivotal_bs &lt;- function(x, q){ mean_x &lt;- mean(x) q0 &lt;- 1 - q raw_quantiles &lt;- quantile(x, q0) true_quantiles &lt;- 2 * mean_x - raw_quantiles names(true_quantiles) &lt;- scales::percent(q) return(true_quantiles) } gg_pred_hist &lt;- function(bs, true_line=TRUE, df0=vote_df){ holdout_year &lt;- bs@holdout_year print(&quot;bootstrapped predictions&quot;) pred_total &lt;- bs@test_sample %&gt;% group_by(sim) %&gt;% summarise(n_dem_wins = sum(pred_samp &gt; 0.5)) %&gt;% group_by() print(paste(&quot;Total NA:&quot;, sum(is.na(pred_total$n_dem_wins)))) pred_total &lt;- pred_total %&gt;% filter(!is.na(n_dem_wins)) print(mean(pred_total$n_dem_wins)) print(pivotal_bs(pred_total$n_dem_wins, c(0.025, 0.2, 0.8, 0.975))) if(true_line){ print(&quot;actual results&quot;) true_dem_wins &lt;- df0[df0$year == holdout_year,] %&gt;% with(sum(sth_pctdem &gt; 0.5)) print(true_dem_wins) gg_annotation &lt;- function() annotate( geom=&quot;text&quot;, x = true_dem_wins, y = 10, angle = 90, hjust = 0, vjust = 1.1, label = paste(&quot;True outcome =&quot;, true_dem_wins) ) } else { true_dem_wins &lt;- numeric(0) gg_annotation &lt;- geom_blank } ggplot(pred_total, aes(x = n_dem_wins, fill = n_dem_wins &lt; 101.5)) + geom_histogram(binwidth = 1) + geom_vline(xintercept = true_dem_wins) + gg_annotation() + ggtitle(paste(&quot;Predicted Democratic seats in&quot;, holdout_year)) + xlab(&quot;N Democratic Seats&quot;) + ylab(&quot;N Bootstrap Sims&quot;) + theme_sixtysix() + scale_fill_manual( values = c(`TRUE`=strong_red, `FALSE` = strong_blue), guide = FALSE ) } gg_pred_hist(bs) ## [1] &quot;bootstrapped predictions&quot; ## [1] &quot;Total NA: 0&quot; ## [1] 83.842 ## 2.5% 20.0% 80.0% 97.5% ## 75.684 80.684 86.684 91.684 ## [1] &quot;actual results&quot; ## [1] 82 We predicted Democrats would win 84 out of 203 seats, with a 95% CI of [76,92]. They actually won 82. _sunglassesemoji_ 4.5 Results for all years Ok, now let’s do the whole darn thing. We will run the same bootstrap above, but iterating through holding out each year. bs_years &lt;- list() for(holdout in seq(2004, 2016, 2)){ print(paste(&quot;###&quot;, holdout, &quot;###&quot;)) bs_years[[as.character(holdout)]] &lt;- bootstrap( vote_df, holdout, conditions, verbose=FALSE ) } ## [1] &quot;### 2004 ###&quot; ## [1] &quot;### 2006 ###&quot; ## [1] &quot;### 2008 ###&quot; ## [1] &quot;### 2010 ###&quot; ## [1] &quot;### 2012 ###&quot; ## [1] &quot;### 2014 ###&quot; ## [1] &quot;### 2016 ###&quot; Let’s check the results! for(holdout in seq(2004, 2016, 2)){ gg_pred_hist(bs_years[[as.character(holdout)]]) %&gt;% print() } ## [1] &quot;bootstrapped predictions&quot; ## [1] &quot;Total NA: 0&quot; ## [1] 95.972 ## 2.5% 20.0% 80.0% 97.5% ## 86.944 91.944 99.944 104.944 ## [1] &quot;actual results&quot; ## [1] 94 ## [1] &quot;bootstrapped predictions&quot; ## [1] &quot;Total NA: 0&quot; ## [1] 102.502 ## 2.5% 20.0% 80.0% 97.5% ## 83.954 96.004 109.004 116.529 ## [1] &quot;actual results&quot; ## [1] 102 ## [1] &quot;bootstrapped predictions&quot; ## [1] &quot;Total NA: 1&quot; ## [1] 114.2345 ## 2.5% 20.0% 80.0% 97.5% ## 100.4689 108.4689 118.4689 133.0189 ## [1] &quot;actual results&quot; ## [1] 104 ## [1] &quot;bootstrapped predictions&quot; ## [1] &quot;Total NA: 0&quot; ## [1] 103.45 ## 2.5% 20.0% 80.0% 97.5% ## 96.375 100.900 105.900 109.900 ## [1] &quot;actual results&quot; ## [1] 91 ## [1] &quot;bootstrapped predictions&quot; ## [1] &quot;Total NA: 0&quot; ## [1] 90.664 ## 2.5% 20.0% 80.0% 97.5% ## 82.803 87.328 94.328 101.853 ## [1] &quot;actual results&quot; ## [1] 92 ## [1] &quot;bootstrapped predictions&quot; ## [1] &quot;Total NA: 0&quot; ## [1] 86.854 ## 2.5% 20.0% 80.0% 97.5% ## 77.708 82.708 90.708 98.233 ## [1] &quot;actual results&quot; ## [1] 84 ## [1] &quot;bootstrapped predictions&quot; ## [1] &quot;Total NA: 0&quot; ## [1] 84.054 ## 2.5% 20.0% 80.0% 97.5% ## 76.583 81.108 87.108 91.633 ## [1] &quot;actual results&quot; ## [1] 82 All of the years look okay… oof, except for 2010. That’s bad. We predicted a Democratic win of 103-100, and in actuality Republicans won 112-91. Look at the race level predictions that year: gg_race_pred(bs_years[[&#39;2010&#39;]]) gg_race_scatter(bs_years[[&#39;2010&#39;]]) map_residuals(bs_years[[&#39;2010&#39;]], &quot;contested&quot;) map_residuals(bs_years[[&#39;2010&#39;]], &quot;previously_uncontested&quot;) We aren’t getting any single race terribly wrong, but we systematically over-predict Democratic performance across the board. What especially hurts is the middle of the plot, where we predict 60-40 Democratic wins and Republicans won 60-40 instead. What happened? Let’s look at the coefficients of the models, by year. coef_df &lt;- do.call( rbind, lapply( bs_years, function(bs) { bs@coefs %&gt;% group_by(term, condition) %&gt;% summarise(estimate=mean(estimate)) %&gt;% mutate(year = bs@holdout_year) } ) ) ggplot( coef_df, aes(x = interaction(condition, term), y=estimate) ) + geom_point(aes(color = (year == 2010))) + theme(axis.text.x = element_text(angle = 90, hjust = 1)) When we held out 2010, we estimated the single highest intercept (good for the Democrats), along with the lowest correlations of USC results and prior STH races. Then, when use those coefficients in 2010, a good year for Republicans, the model totall overshoots. Darn. So what do we do? We have a few options. Do nothing. When we predict for 2018, 2010 will be in our training set, so we will have that coverage. Hopefully, 2018 won’t be an outlier given 2004-2016 the way 2010 was for 2004-2016. FiveThirtyEight projects a Democratic USC win in 2018, but not out of line of other results we’ve seen, in 2006 ad 2008. Be Bayesian. Model the annual random effects and throw on a strong prior. This is my instinct, but I’m committed to being Frequentist for the workshop :) Rejigger the formulas we use. While it’s a cardinal sin to retouch your model after evaluating a test set, we don’t have enough years to use best practices (ideally we would create a super-holdout of years that we never looked at until the very end. But I’m not willing to throw out years when we only have seven). If we do this, we need to be intellectually honest with ourselves, and be super duper careful not to overfit the model. I attempt to achieve this by limiting myself to specifications that would only be a priori obvious, and that use only as many or fewer degrees of freedom as what I’ve already fit. Let’s do option one. We won’t touch our model, and we’ll hope that since 2010 is in our training data now, 2018 won’t be a bad outlier. For completeness, let’s revisit the maps of residuals. Does it look like we systematically mis-predict a given region? for(bs in bs_years){ for(condname in c(&quot;contested&quot;, &quot;previously_uncontested&quot;)){ map_residuals(bs, condname) %&gt;% print } } The biggest pattern is that we miss a bunch of races in a year all in the same way. It looks like maybe we also miss Philadelphia all in the same direction when we do (though notably, that’s not true in e.g. 2012). Done with the due diligence. Let’s predict! 4.6 Predicting 2018 We’ll predict the 2018 election. We’ve already prepped the 2018 equivalent of df in Prepping the Data for Analysis. load(&quot;outputs/df_2018.rda&quot;) df_2018 &lt;- df_2018 %&gt;% add_cols() df_2018 &lt;- add_condition(df_2018, conditions) vote_df &lt;- bind_rows(vote_df, df_2018) pred_2018 &lt;- bootstrap( vote_df, holdout_year=2018, conditions=conditions ) ## [1] 100 ## [1] 200 ## [1] 300 ## [1] 400 ## [1] 500 gg_race_pred(pred_2018) gg_pred_hist(pred_2018, true_line = FALSE) ## [1] &quot;bootstrapped predictions&quot; ## [1] &quot;Total NA: 0&quot; ## [1] 88.434 ## 2.5% 20.0% 80.0% 97.5% ## 76.868 83.868 93.068 97.868 We predict Republicans to win the house 215-88, with a 95% CI on Dem seats of [76 - 97]. Republicans win the majority in 98.8% of bootstraps. 4.7 Fast forward to today Ok, we’re not actually in October 2018. We know who actually won. How did these predictions do? results_2018 &lt;- read.csv( &quot;data/UnOfficial_11232018092637AM.csv&quot; ) %&gt;% mutate( vote_total = asnum(gsub(&quot;\\\\,&quot;,&quot;&quot;,Votes)), sth = sprintf(&quot;%03d&quot;, asnum(gsub(&quot;^([0-9]+)[a-z].*&quot;, &quot;\\\\1&quot;, District.Name))), party = ifelse( Party.Name == &quot;Democratic&quot;, &quot;DEM&quot;, ifelse(Party.Name == &quot;Republican&quot;, &quot;REP&quot;, NA) ) ) %&gt;% mutate( party = replace( party, Candidate.Name %in% c( &quot;BERNSTINE, AARON JOSEPH &quot;, &quot;SANKEY, THOMAS R III&quot;, &quot;GABLER, MATTHEW M &quot;), &quot;REP&quot; ), party = replace(party, Candidate.Name == &quot;LONGIETTI, MARK ALFRED &quot;, &quot;DEM&quot;) ) %&gt;% filter(!is.na(party)) %&gt;% group_by(sth, party) %&gt;% summarise(votes = sum(vote_total)) %&gt;% group_by(sth) %&gt;% summarise(sth_pctdem = sum(votes * (party == &quot;DEM&quot;)) / sum(votes)) vote_df &lt;- left_join(vote_df, results_2018, by = &quot;sth&quot;, suffix = c(&quot;&quot;,&quot;.2018&quot;)) %&gt;% mutate( sth_pctdem = ifelse( substr(race,1,4)==&quot;2018&quot;, sth_pctdem.2018, sth_pctdem ) ) %&gt;% select(-sth_pctdem.2018) gg_pred_hist(pred_2018) ## [1] &quot;bootstrapped predictions&quot; ## [1] &quot;Total NA: 0&quot; ## [1] 88.434 ## 2.5% 20.0% 80.0% 97.5% ## 76.868 83.868 93.068 97.868 ## [1] &quot;actual results&quot; ## [1] 93 Democrats won 93 State House seats, right in the heart of our prediction. gg_race_pred(pred_2018) These look ok, but we underpredicted Democrats in strong Republican districts. Democrats won a bunch of seats that we thought were going to be 60-40 Republican wins. Remember when I said you would know it when we saw a bad residual percentile histogram? gg_resid(pred_2018) We did well in the previously uncontested models, but systematically underpredicted the contested model. Such is the effect of year-level random effects. That’s it! I’m going to claim victory: the real result was right in the heart of our distribution. See you in 2020! "]
]
